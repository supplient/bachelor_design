{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "segment_cal_dis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supplient/bachelor_design/blob/test_old_code/segment_cal_dis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq1mSA3o1paX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "4be14a53-77f8-4421-c2a7-8f848c33eca3"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import jieba\n",
        "import re\n",
        "import types\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.merge import concatenate, add, multiply\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "import io\n",
        "import sys\n",
        "import urllib.request\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny6eRIJr1pap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 创建停用词list  \n",
        "def stopwordslist(stop_file):  \n",
        "    stopwords = [line.strip() for line in open(stop_file, 'r').readlines()]  \n",
        "    return stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWNvokjH1paw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_word_edit(lhs, rhs):\n",
        "    len_1 = len(lhs)\n",
        "    len_2 = len(rhs)\n",
        "    dist_table = [[0] * (len_2 + 1) for i in range(len_1 + 1)]\n",
        "    for i in range(len_1 + 1):\n",
        "        dist_table[i][0] = i\n",
        "    for j in range(len_2 + 1):\n",
        "        dist_table[0][j] = j\n",
        "    for i in range(1, len_1 + 1):\n",
        "        for j in range(1, len_2 + 1):\n",
        "            if lhs[i - 1] == rhs[j - 1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "            deletion = dist_table[i - 1][j] + 1\n",
        "            insertion = dist_table[i][j - 1] + 1\n",
        "            substitution = dist_table[i - 1][j - 1] + cost\n",
        "            dist_table[i][j] = min(min(deletion, insertion), substitution)\n",
        "    return dist_table[len_1][len_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zTTcFa1pa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 对句子进行分词;赋予标签\n",
        "def seg_sentence(input_file, log_file, stop_file, segwithoutlabel_file):\n",
        "    words = []\n",
        "    labels = []\n",
        "    text1 = []\n",
        "    outstr=''\n",
        "    stopwords = stopwordslist(stop_file)  # 这里加载停用词的路径  \n",
        "    outf = open(log_file, 'w', encoding='utf-8', errors='ignore')\n",
        "    outft = open(segwithoutlabel_file, 'w', encoding='utf-8', errors='ignore')\n",
        "    with open(input_file,'r', encoding='utf-8', errors='ignore') as inf:\n",
        "        for line in inf.readlines():\n",
        "            line = line.strip()\n",
        "            # print (\"line:\",line)\n",
        "            if line:\n",
        "                t_vec = line.split('\\t')\n",
        "                if len(t_vec) >= 3:\n",
        "                    dec = t_vec[0]\n",
        "                    dec = dec.strip()\n",
        "                    count_name = t_vec[1]\n",
        "                    label = str(t_vec[2])\n",
        "                    if label == 'ILf':\n",
        "                        label = 'ILF'\n",
        "                    if label == 'EIf':\n",
        "                        label = 'EIF'\n",
        "                else:\n",
        "                    continue\n",
        "            labels.append(label)\n",
        "            # print (\"dec\", dec)\n",
        "            # print (\"count_name\", count_name)\n",
        "            # print (\"label\", label)\n",
        "            sentence_seged = jieba.cut(dec)\n",
        "            count_seged = jieba.cut(count_name)\n",
        "            sentence_result = []\n",
        "            count_result = []\n",
        "            for word in sentence_seged:\n",
        "                sentence_result.append(word)\n",
        "            for count_word in count_seged:\n",
        "                count_result.append(count_word)\n",
        "            text2 = []\n",
        "            # print (\"sentence_result\", sentence_result)\n",
        "            # print (\"count_result\", count_result)\n",
        "            for word in sentence_result:\n",
        "                flag = 'O'\n",
        "                text1 = []\n",
        "                if word not in stopwords:\n",
        "                    #print (\"stopwords\")\n",
        "                    if word != '\\t' and word != '' and word != ' ':\n",
        "                        #print (\"kongbai\")\n",
        "                        for count_e in count_result:\n",
        "                            #print (\"daozhelile\")\n",
        "                            if count_e == '':\n",
        "                                #print (\"count_word\", count_e)\n",
        "                                continue\n",
        "                            else:\n",
        "                                if word == count_e:\n",
        "                                    flag = label\n",
        "                                    break\n",
        "                                else:\n",
        "                                    # print (\"word\", word)\n",
        "                                    # print (\"count_word\", count_e)\n",
        "                                    # print (\"length\", len(count_e))\n",
        "                                    dist = cal_word_edit(word, count_e)\n",
        "                                    min_len = min(len(word), len(count_e))\n",
        "                                    if min_len <= 3:\n",
        "                                        if dist < 2:\n",
        "                                            flag = label\n",
        "                                            break\n",
        "                                    elif min_len <= 7:\n",
        "                                        if dist < 3:\n",
        "                                            flag = label\n",
        "                                            break\n",
        "                                    else:\n",
        "                                        if dist < 4:\n",
        "                                            flag = label\n",
        "                                            break\n",
        "                        i = 0\n",
        "                        for str_tmp in word:\n",
        "                            if flag == 'O':\n",
        "                                str_flag = 'O'\n",
        "                            elif i == 0:\n",
        "                                str_flag = 'B' + '-' + flag\n",
        "                            else:\n",
        "                                str_flag = 'I' + '-' + flag\n",
        "                            i += 1\n",
        "                            text1.append(str_tmp)\n",
        "                            text1.append(str_flag)\n",
        "                            text2.append(str_tmp)\n",
        "                            outstr = str_tmp.strip()\n",
        "                            outstr = outstr.strip(' ')\n",
        "                            outf.write(outstr)\n",
        "                            outft.write(outstr)\n",
        "                            outf.write(\" \")\n",
        "                            outft.write(\" \")\n",
        "                            outf.write(str_flag)\n",
        "                            outf.write(\"\\n\")\n",
        "            outf.write(\"]\")\n",
        "            outf.write(\"\\n\")\n",
        "                        #print (\"line:\",outstr)\n",
        "    outf.close()\n",
        "    outft.close()\n",
        "    return text2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go-YW3TR1pbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "locale = False\n",
        "addh = \"/gdrive\"\n",
        "if locale:\n",
        "    addh = \"/mnt/d\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE28Pybi16lq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "5fb5536d-5018-4949-efe5-46e0b44fdb19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMOTUgP11pbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a88a0880-6cdf-437f-8da1-5a3c4c1300b4"
      },
      "source": [
        "train_text=seg_sentence(\n",
        "    addh+'/My Drive/Graduation/old/实验/summary_2.txt',\n",
        "    addh+'/My Drive/Graduation/old/实验/test_data_e.data',\n",
        "    addh+'/My Drive/Graduation/old/实验/stopwords.txt',\n",
        "    addh+'/My Drive/Graduation/old/实验/segwithoutlabel_file.txt'\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.938 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}